//
//     Generated by class-dump 3.5 (64 bit).
//
//     class-dump is Copyright (C) 1997-1998, 2000-2001, 2004-2013 by Steve Nygard.
//
@import Foundation;
#pragma mark Blocks

typedef void (^CDUnknownBlockType)(void); // return type and parameters are unknown

#pragma mark -

//
// File: /System/Library/PrivateFrameworks/Speech.framework/Versions/A/Speech
// UUID: EA338612-C837-3E9E-A058-E60105B0FBBD
//
//                           Arch: x86_64
//                Current version: 1.0.0
//          Compatibility version: 1.0.0
//                 Source version: 54.0.0.0.0
//       Minimum Mac OS X version: 10.12.0
//                    SDK version: 10.12.0
//
// Objective-C Garbage Collection: Unsupported
//

@protocol AFDictationDelegate <NSObject>

//@optional
//- (void)dictationConnection:(AFDictationConnection *)arg1 didReceiveSearchResults:(NSArray *)arg2 recognizedText:(NSString *)arg3 stable:(BOOL)arg4 final:(BOOL)arg5;
//- (void)dictationConnection:(AFDictationConnection *)arg1 didFinishWritingAudioFile:(NSFileHandle *)arg2 error:(NSError *)arg3;
//- (void)dictationConnnectionDidChangeAvailability:(AFDictationConnection *)arg1;
//- (void)dictationConnection:(AFDictationConnection *)arg1 didRecognizeTranscriptionObjects:(NSArray *)arg2 languageModel:(NSString *)arg3;
//- (void)dictationConnectionSpeechRecognitionDidSucceed:(AFDictationConnection *)arg1;
//- (void)dictationConnection:(AFDictationConnection *)arg1 didProcessAudioDuration:(double)arg2;
//- (void)dictationConnection:(AFDictationConnection *)arg1 didRecognizeTokens:(NSArray *)arg2 languageModel:(NSString *)arg3;
//- (void)dictationConnection:(AFDictationConnection *)arg1 didRecognizePhrases:(NSArray *)arg2 languageModel:(NSString *)arg3 correctionIdentifier:(id)arg4;
//- (void)dictationConnection:(AFDictationConnection *)arg1 speechRecognitionDidFail:(NSError *)arg2;
//- (void)dictationConnection:(AFDictationConnection *)arg1 speechRecordingDidFail:(NSError *)arg2;
//- (void)dictationConnectionSpeechRecordingDidCancel:(AFDictationConnection *)arg1;
//- (void)dictationConnectionSpeechRecordingDidEnd:(AFDictationConnection *)arg1;
//- (void)dictationConnectionSpeechRecordingDidBegin:(AFDictationConnection *)arg1;
//- (void)dictationConnectionSpeechRecordingWillBegin:(AFDictationConnection *)arg1;
@end

@interface SFTranscriptionSegment : NSObject <NSCopying, NSSecureCoding>
{
    float _confidence;
    NSString *_substring;
    double _timestamp;
    double _duration;
    NSArray *_alternativeSubstrings;
    struct _NSRange _substringRange;
}
+ (BOOL)supportsSecureCoding;
@property(readonly, nonatomic) NSArray *alternativeSubstrings; // @synthesize alternativeSubstrings=_alternativeSubstrings;
@property(readonly, nonatomic) float confidence; // @synthesize confidence=_confidence;
@property(readonly, nonatomic) double duration; // @synthesize duration=_duration;
@property(readonly, nonatomic) double timestamp; // @synthesize timestamp=_timestamp;
@property(readonly, nonatomic) struct _NSRange substringRange; // @synthesize substringRange=_substringRange;
@property(readonly, copy, nonatomic) NSString *substring; // @synthesize substring=_substring;
- (id)_initWithSubstring:(id)arg1 range:(struct _NSRange)arg2 timestamp:(double)arg3 duration:(double)arg4 confidence:(float)arg5 alternativeSubstrings:(id)arg6;
- (id)initWithCoder:(id)arg1;
- (void)encodeWithCoder:(id)arg1;
- (id)description;
- (id)copyWithZone:(struct _NSZone *)arg1;
- (BOOL)isEqual:(id)arg1;
- (unsigned long long)hash;

@end

@interface SFTranscription : NSObject <NSCopying, NSSecureCoding>
{
    NSString *_formattedString;
    NSArray *_segments;
}

+ (BOOL)supportsSecureCoding;
@property(readonly, copy, nonatomic) NSArray *segments; // @synthesize segments=_segments;
@property(readonly, copy, nonatomic) NSString *formattedString; // @synthesize formattedString=_formattedString;
- (id)_initWithSegments:(id)arg1 formattedString:(id)arg2;
- (void)encodeWithCoder:(id)arg1;
- (id)initWithCoder:(id)arg1;
- (id)copyWithZone:(struct _NSZone *)arg1;
- (id)description;
- (BOOL)isEqual:(id)arg1;
- (unsigned long long)hash;

@end

@protocol NSCoding
- (id)initWithCoder:(NSCoder *)arg1;
- (void)encodeWithCoder:(NSCoder *)arg1;
@end

@protocol NSCopying
- (id)copyWithZone:(struct _NSZone *)arg1;
@end

//@protocol NSObject
//@property(readonly, copy) NSString *description;
//@property(readonly) Class superclass;
//@property(readonly) unsigned long long hash;
//- (struct _NSZone *)zone;
//- (unsigned long long)retainCount;
//- (id)autorelease;
//- (oneway void)release;
//- (id)retain;
//- (BOOL)respondsToSelector:(SEL)arg1;
//- (BOOL)conformsToProtocol:(Protocol *)arg1;
//- (BOOL)isMemberOfClass:(Class)arg1;
//- (BOOL)isKindOfClass:(Class)arg1;
//- (BOOL)isProxy;
//- (id)performSelector:(SEL)arg1 withObject:(id)arg2 withObject:(id)arg3;
//- (id)performSelector:(SEL)arg1 withObject:(id)arg2;
//- (id)performSelector:(SEL)arg1;
//- (id)self;
//- (Class)class;
//- (BOOL)isEqual:(id)arg1;
//
//@optional
//@property(readonly, copy) NSString *debugDescription;
//@end

@protocol NSSecureCoding <NSCoding>
+ (BOOL)supportsSecureCoding;
@end

@protocol SFSpeechRecognitionBufferDelegate <NSObject>
- (void)stopSpeech;
- (void)addRecordedSpeechSampleData:(NSData *)arg1;
@end

@interface _SFSearchResult : NSObject
{
    NSHTTPURLResponse *_response;
    NSData *_data;
    long long _searchType;
}



@property(readonly, nonatomic) long long searchType; // @synthesize searchType=_searchType;
@property(readonly, nonatomic) NSData *data; // @synthesize data=_data;
@property(readonly, nonatomic) NSHTTPURLResponse *response; // @synthesize response=_response;
- (id)description;
- (id)initWithVoiceSearchResult:(id)arg1;

@end

@interface _SFSearchRequest : NSObject
{
    long long _searchTypes;
    NSDictionary *_headerFields;
    NSDictionary *_queryParameters;
}

@property(copy, nonatomic) NSDictionary *queryParameters; // @synthesize queryParameters=_queryParameters;
@property(copy, nonatomic) NSDictionary *headerFields; // @synthesize headerFields=_headerFields;
@property(nonatomic) long long searchTypes; // @synthesize searchTypes=_searchTypes;

@end

@interface SFSpeechRecognitionRequest : NSObject
{
    BOOL _forceOfflineRecognition;
    BOOL _shouldReportPartialResults;
    BOOL _detectMultipleUtterances;
    double _maxiumRecognitionDuration;
    _SFSearchRequest *_searchRequest;
    NSDictionary *_voiceTriggerEventInfo;
    long long _taskHint;
    NSArray *_contextualStrings;
    NSString *_interactionIdentifier;
}
@property(copy, nonatomic) NSString *interactionIdentifier; // @synthesize interactionIdentifier=_interactionIdentifier;
@property(copy, nonatomic) NSArray *contextualStrings; // @synthesize contextualStrings=_contextualStrings;
@property(nonatomic) BOOL detectMultipleUtterances; // @synthesize detectMultipleUtterances=_detectMultipleUtterances;
@property(nonatomic) BOOL shouldReportPartialResults; // @synthesize shouldReportPartialResults=_shouldReportPartialResults;
@property(nonatomic) long long taskHint; // @synthesize taskHint=_taskHint;
@property(retain, nonatomic, getter=_voiceTriggerEventInfo, setter=_setVoiceTriggerEventInfo:) NSDictionary *_voiceTriggerEventInfo; // @synthesize _voiceTriggerEventInfo;
@property(retain, nonatomic, getter=_searchRequest, setter=_setSearchRequest:) _SFSearchRequest *_searchRequest; // @synthesize _searchRequest;
@property(nonatomic, getter=_forceOfflineRecognition, setter=_setForceOfflineRecognition:) BOOL _forceOfflineRecognition; // @synthesize _forceOfflineRecognition;
@property(nonatomic, getter=_maximumRecognitionDuration, setter=_setMaximumRecognitionDuration:) double _maxiumRecognitionDuration; // @synthesize _maxiumRecognitionDuration;
- (id)_speechRequestOptions;
- (id)_dictationOptionsWithTaskHint:(long long)arg1 requestIdentifier:(id)arg2;
- (id)_startedConnectionWithLanguageCode:(id)arg1 delegate:(id)arg2 taskHint:(long long)arg3 requestIdentifier:(id)arg4;
- (BOOL)_powerMeteringAvailable;
- (BOOL)automaticallyEndpoint;
- (id)init;
- (id)_searchRequests;
- (void)_setSearchRequests:(id)arg1;

@end


@interface SFSpeechRecognitionTask : NSObject <AFDictationDelegate, SFSpeechRecognitionBufferDelegate>
{
//    AFDictationConnection *_dictationConnection;
    NSOperationQueue *_externalQueue;
    NSString *_languageCode;
    SFSpeechRecognitionRequest *_request;
//    NSObject<OS_dispatch_queue> *_internalQueue;
    BOOL _completed;
    BOOL _running;
    BOOL _finishing;
    BOOL _cancelled;
    BOOL _powerAvailable;
    long long _taskHint;
    NSError *_error;
    NSString *_requestIdentifier;
}

+ (id)recognizedUtteranceFromSpeechPhrases:(id)arg1 final:(BOOL)arg2;
+ (id)transcriptionsWithTokens:(id)arg1;
@property(readonly, copy, nonatomic) NSString *requestIdentifier; // @synthesize requestIdentifier=_requestIdentifier;
@property(readonly, nonatomic, getter=isPowerAvailable) BOOL powerAvailable; // @synthesize powerAvailable=_powerAvailable;
@property(readonly, copy, nonatomic) NSError *error; // @synthesize error=_error;
@property(readonly, nonatomic, getter=isCancelled) BOOL cancelled; // @synthesize cancelled=_cancelled;
@property(readonly, nonatomic, getter=isFinishing) BOOL finishing; // @synthesize finishing=_finishing;
@property(readonly, nonatomic) long long _taskHint; // @synthesize _taskHint;
- (void)stopSpeech;
- (void)addRecordedSpeechSampleData:(id)arg1;
- (void)dictationConnectionSpeechRecognitionDidSucceed:(id)arg1;
- (void)dictationConnection:(id)arg1 speechRecognitionDidFail:(id)arg2;
- (void)dictationConnection:(id)arg1 speechRecordingDidFail:(id)arg2;
- (void)dictationConnectionSpeechRecordingDidCancel:(id)arg1;
- (void)dictationConnectionSpeechRecordingDidEnd:(id)arg1;
- (void)dictationConnectionSpeechRecordingDidBegin:(id)arg1;
@property(readonly, nonatomic) float averagePower;
@property(readonly, nonatomic) float peakPower;
- (void)cancel;
- (void)finish;
@property(readonly, nonatomic) long long state;
- (id)_initWithRequest:(id)arg1 queue:(id)arg2 languageCode:(id)arg3 taskHint:(long long)arg4;

// Remaining properties
@property(readonly, copy) NSString *debugDescription;
@property(readonly, copy) NSString *description;
@property(readonly) unsigned long long hash;
@property(readonly) Class superclass;

@end



@interface _SFSpeechRecognitionBlockTask : SFSpeechRecognitionTask
{
    CDUnknownBlockType _resultHandler;
}

- (void)dictationConnection:(id)arg1 didRecognizeTokens:(id)arg2 languageModel:(id)arg3;
- (void)dictationConnection:(id)arg1 didRecognizePhrases:(id)arg2 languageModel:(id)arg3 correctionIdentifier:(id)arg4;
- (void)dictationConnectionSpeechRecognitionDidSucceed:(id)arg1;
- (void)dictationConnection:(id)arg1 speechRecognitionDidFail:(id)arg2;
- (void)dictationConnection:(id)arg1 speechRecordingDidFail:(id)arg2;
- (void)_finalizeResultHandler;
- (void)_fireResultHandlerWithResult:(id)arg1 error:(id)arg2;
- (id)_initWithRequest:(id)arg1 queue:(id)arg2 languageCode:(id)arg3 taskHint:(long long)arg4 resultHandler:(CDUnknownBlockType)arg5;

@end

@interface SFSpeechRecognitionResult : NSObject <NSCopying, NSSecureCoding>
{
    NSArray *_transcriptions;
    BOOL _final;
    SFTranscription *_bestTranscription;
}

+ (BOOL)supportsSecureCoding;
@property(readonly, nonatomic, getter=isFinal) BOOL final; // @synthesize final=_final;
@property(readonly, copy, nonatomic) SFTranscription *bestTranscription; // @synthesize bestTranscription=_bestTranscription;
@property(readonly, copy, nonatomic) NSArray *transcriptions;
- (id)_initWithBestTranscription:(id)arg1 final:(BOOL)arg2;
- (void)encodeWithCoder:(id)arg1;
- (id)initWithCoder:(id)arg1;
- (id)description;
- (id)copyWithZone:(struct _NSZone *)arg1;
- (BOOL)isEqual:(id)arg1;
- (unsigned long long)hash;

@end

@interface _SFSpeechRecognitionDelegateTask : SFSpeechRecognitionTask
{
//    id <_SFSpeechRecognitionTaskDelegatePrivate> _delegate;
    SFSpeechRecognitionResult *_recognitionResultToReportAfterFinalSearchResults;
    _SFSpeechRecognitionDelegateTask *_selfReference;
    BOOL _waitForVoiceSearchResult;
    BOOL _hasSentRealSearchResults;
}

- (void)dictationConnection:(id)arg1 didReceiveSearchResults:(id)arg2 recognizedText:(id)arg3 stable:(BOOL)arg4 final:(BOOL)arg5;
- (void)dictationConnection:(id)arg1 didProcessAudioDuration:(double)arg2;
- (void)dictationConnection:(id)arg1 didRecognizeTokens:(id)arg2 languageModel:(id)arg3;
- (void)dictationConnection:(id)arg1 didRecognizePhrases:(id)arg2 languageModel:(id)arg3 correctionIdentifier:(id)arg4;
- (void)dictationConnectionSpeechRecognitionDidSucceed:(id)arg1;
- (void)dictationConnection:(id)arg1 speechRecognitionDidFail:(id)arg2;
- (void)dictationConnectionSpeechRecordingDidEnd:(id)arg1;
- (void)dictationConnection:(id)arg1 speechRecordingDidFail:(id)arg2;
- (void)_tellDelegateDidFinishSuccessfully:(BOOL)arg1;
- (void)dictationConnectionSpeechRecordingDidCancel:(id)arg1;
- (id)_initWithRequest:(id)arg1 queue:(id)arg2 languageCode:(id)arg3 taskHint:(long long)arg4 delegate:(id)arg5;

@end



@interface SFSpeechRecognizer : NSObject <AFDictationDelegate>
{
//    AFDictationConnection *_dictationConnection;
    NSString *_languageCode;
    id <NSObject> _facetimeObserver;
    id <NSObject> _foregroundObserver;
    id <NSObject> _preferencesObserver;
    NSLocale *_locale;
//    id <SFSpeechRecognizerDelegate> _delegate;
    long long _defaultTaskHint;
    NSOperationQueue *_queue;
}

+ (void)_fetchSupportedForcedOfflineLocalesWithCompletion:(CDUnknownBlockType)arg1;
+ (void)requestAuthorization:(CDUnknownBlockType)arg1;
+ (long long)authorizationStatus;
+ (id)supportedLocales;
+ (void)initialize;
@property(retain, nonatomic) NSOperationQueue *queue; // @synthesize queue=_queue;
@property(nonatomic) long long defaultTaskHint; // @synthesize defaultTaskHint=_defaultTaskHint;
//@property(nonatomic) __weak id <SFSpeechRecognizerDelegate> delegate; // @synthesize delegate=_delegate;
@property(readonly, copy, nonatomic) NSLocale *locale; // @synthesize locale=_locale;
- (void)_informDelegateOfPreferencesChange;
- (void)_informDelegateOfAvailabilityChange;
- (void)dictationConnnectionDidChangeAvailability:(id)arg1;
- (void)_sendEngagementFeedback:(long long)arg1 requestIdentifier:(id)arg2;
- (id)recognitionTaskWithRequest:(id)arg1 delegate:(id)arg2;
- (id)recognitionTaskWithRequest:(id)arg1 resultHandler:(CDUnknownBlockType)arg2;
- (id)_recognitionTaskWithResultHandler:(CDUnknownBlockType)arg1;
- (void)prepareWithRequest:(id)arg1;
- (BOOL)_isInternalTaskHint:(long long)arg1;
- (void)_requestOfflineDictationSupportWithCompletion:(CDUnknownBlockType)arg1;
@property(readonly, nonatomic, getter=_isAvailableForForcedOfflineRecognition) BOOL _availableForForcedOfflineRecognition;
@property(readonly, nonatomic, getter=isAvailableForRecordingRecognition) BOOL availableForRecordingRecognition;
@property(readonly, nonatomic, getter=isAvailable) BOOL available;
- (void)dealloc;
- (id)initWithLocale:(id)arg1;
- (id)init;

// Remaining properties
@property(readonly, copy) NSString *debugDescription;
@property(readonly, copy) NSString *description;
@property(readonly) unsigned long long hash;
@property(readonly) Class superclass;

@end











@interface SFSpeechRecordingRecognitionRequest : SFSpeechRecognitionRequest
{
    BOOL _automaticallyEndpoint;
    BOOL _shouldPlayAudioRecordSounds;
    double _maximumRecordingDuration;
}

@property(nonatomic) double maximumRecordingDuration; // @synthesize maximumRecordingDuration=_maximumRecordingDuration;
@property(nonatomic) BOOL shouldPlayAudioRecordSounds; // @synthesize shouldPlayAudioRecordSounds=_shouldPlayAudioRecordSounds;
@property(nonatomic) BOOL automaticallyEndpoint; // @synthesize automaticallyEndpoint=_automaticallyEndpoint;
- (id)_startedConnectionWithLanguageCode:(id)arg1 delegate:(id)arg2 taskHint:(long long)arg3 requestIdentifier:(id)arg4;
- (BOOL)_powerMeteringAvailable;
- (id)init;

@end

@interface SFSpeechURLRecognitionRequest : SFSpeechRecognitionRequest
{
    NSURL *_URL;
}

@property(readonly, copy, nonatomic) NSURL *URL; // @synthesize URL=_URL;
- (id)_startedConnectionWithLanguageCode:(id)arg1 delegate:(id)arg2 taskHint:(long long)arg3 requestIdentifier:(id)arg4;
- (id)initWithURL:(id)arg1;
- (id)init;

@end



